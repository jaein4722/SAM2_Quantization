{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b28288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bc90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def save_masks(model_name, index, image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    plt.ioff()\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        os.makedirs(f\"qualitative/{model_name}\", exist_ok=True)\n",
    "        plt.savefig(f\"qualitative/{model_name}/mask_{index}.png\", bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13302d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1b_path = Path(\"../../datasets/benchmark/sa1b\")\n",
    "sa1b_images = glob.glob(str(sa1b_path / \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637580a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../datasets/benchmark/sa1b/sa_10034.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa1b_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb50ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1b_points = {\n",
    "    sa1b_images[0]: [[1000, 600]],\n",
    "    sa1b_images[1]: [[1000, 1200]],\n",
    "    sa1b_images[2]: [[250, 1500]],\n",
    "    sa1b_images[3]: [[1600, 700]],\n",
    "    sa1b_images[4]: [[1250, 1000]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21e227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../../datasets/benchmark/sa1b/sa_10034.jpg': [[1000, 600]],\n",
       " '../../datasets/benchmark/sa1b/sa_10110.jpg': [[1000, 1200]],\n",
       " '../../datasets/benchmark/sa1b/sa_10129.jpg': [[250, 1500]],\n",
       " '../../datasets/benchmark/sa1b/sa_10202.jpg': [[1600, 700]],\n",
       " '../../datasets/benchmark/sa1b/sa_10487.jpg': [[1250, 1000]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa1b_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43702585",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpq_sam2_config = {\n",
    "    \"base_plus\": {\n",
    "        \"model_name\": \"alpq_sam2_base_plus\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_base_plus_20251110_155500/checkpoints/checkpoint.pt\",\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"model_name\": \"alpq_sam2_small\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_small_20251111_172858/checkpoints/checkpoint.pt\",\n",
    "    },\n",
    "    \"tiny\": {\n",
    "        \"model_name\": \"alpq_sam2_tiny\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_tiny_20251112_161453_importancefixed/checkpoints/checkpoint.pt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "sam2_config = {\n",
    "    \"base_plus\": {\n",
    "        \"model_name\": \"sam2_base_plus\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"sam2_checkpoint\": \"../checkpoints/sam2.1_hiera_base_plus.pt\",\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"model_name\": \"sam2_small\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"sam2_checkpoint\": \"../checkpoints/sam2.1_hiera_small.pt\",\n",
    "    },\n",
    "    \"tiny\": {\n",
    "        \"model_name\": \"sam2_tiny\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"sam2_checkpoint\": \"../checkpoints/sam2.1_hiera_tiny.pt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "minmax_sam2_config = {\n",
    "    \"base_plus\": {\n",
    "        \"model_name\": \"minmax_sam2_base_plus\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_minmax/minmax_qat_base_plus_20251111_122542/checkpoints/checkpoint_sam2.pt\",\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"model_name\": \"minmax_sam2_small\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_minmax/minmax_qat_small_20251111_233441/checkpoints/checkpoint_sam2.pt\",\n",
    "    },\n",
    "    \"tiny\": {\n",
    "        \"model_name\": \"minmax_sam2_tiny\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_minmax/minmax_qat_tiny_20251111_165611/checkpoints/checkpoint_sam2.pt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "baseonly_sam2_config = {\n",
    "    \"base_plus\": {\n",
    "        \"model_name\": \"baseonly_sam2_base_plus\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"sam2_checkpoint\": \"../sam2_logs/ablations/adaptive_qat_toy_base_plus_20251112_101653/checkpoints/checkpoint.pt\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1a4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def make_qualitatives(sa1b_path, model_config):\n",
    "    model_cfg = model_config[\"model_cfg\"]\n",
    "    sam2_checkpoint = model_config[\"sam2_checkpoint\"]\n",
    "\n",
    "    sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "    predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "    # Fix KeyError by always using the string path as the key into sa1b_points\n",
    "    for i, sa1b_image in enumerate(sa1b_path.glob(\"*.jpg\")):\n",
    "        key = str(sa1b_image)\n",
    "        if key not in sa1b_points:\n",
    "            print(f\"Warning: {key} not found in sa1b_points, skipping.\")\n",
    "            continue\n",
    "        input_point = np.array(sa1b_points[key])\n",
    "        input_label = np.array([1])\n",
    "        image = Image.open(sa1b_image)\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        sorted_ind = np.argsort(scores)[::-1]\n",
    "        masks = masks[sorted_ind]\n",
    "        scores = scores[sorted_ind]\n",
    "        logits = logits[sorted_ind]\n",
    "        save_masks(model_config[\"model_name\"], i+1, image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a55f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2656061/3384543421.py:49: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 10))\n"
     ]
    }
   ],
   "source": [
    "for model_name in alpq_sam2_config.keys():  \n",
    "    make_qualitatives(sa1b_path, alpq_sam2_config[model_name])\n",
    "    \n",
    "for model_name in sam2_config.keys():\n",
    "    make_qualitatives(sa1b_path, sam2_config[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb75accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in minmax_sam2_config.keys():\n",
    "    make_qualitatives(sa1b_path, minmax_sam2_config[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16004e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in baseonly_sam2_config.keys():\n",
    "    make_qualitatives(sa1b_path, baseonly_sam2_config[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630be98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
