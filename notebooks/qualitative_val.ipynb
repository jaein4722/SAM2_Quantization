{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SA-1B/SA-V Val Split Qualitative\n",
        "\n",
        "이 노트북은 SA-1B val 이미지 5~10장과 SA-V val 비디오 2~3개를 무작위로 추출해 모든 모델 조합의 포인트 프롬프트 결과를 PDF로 저장합니다. 실행 전에 `conda activate SAM2` 환경을 활성화한 뒤 Context7 MCP 설정과 동일한 라이브러리 버전을 사용하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9df3c7f7f0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import font_manager\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from PIL import Image\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "TIMES_FONT_DIR = Path(\"../../Times New Roman\")\n",
        "if TIMES_FONT_DIR.exists():\n",
        "    for font_path in TIMES_FONT_DIR.glob(\"*.ttf\"):\n",
        "        font_manager.fontManager.addfont(str(font_path))\n",
        "else:\n",
        "    print(f\"[WARN] Times New Roman directory not found: {TIMES_FONT_DIR}\")\n",
        "\n",
        "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"savefig.facecolor\"] = \"white\"\n",
        "plt.rcParams[\"font.family\"] = \"Times New Roman Cyr\"\n",
        "plt.rcParams[\"font.sans-serif\"] = [\"Times New Roman Cyr\"]\n",
        "plt.rcParams[\"font.serif\"] = [\"Times New Roman Cyr\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    autocast_context = torch.autocast(\"cuda\", dtype=torch.bfloat16)\n",
        "    autocast_context.__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "GLOBAL_SEED = 2024\n",
        "random.seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SA-1B val root: ../../datasets/sa-1b_split/val\n",
            "SA-V val root : ../../datasets/sa-v/sav_val/JPEGImages_24fps\n",
            "Image PDF : /home/lji/SAM/sam2/qualitative_val_outputs/qualitative_val_images.pdf\n",
            "Video PDF : /home/lji/SAM/sam2/qualitative_val_outputs/qualitative_val_videos.pdf\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = Path(\"../../datasets\")\n",
        "SA1B_VAL_ROOT = DATA_ROOT / \"sa-1b_split\" / \"val\"\n",
        "SAV_VAL_FRAMES = DATA_ROOT / \"sa-v\" / \"sav_val\" / \"JPEGImages_24fps\"\n",
        "SAV_VAL_ANN = DATA_ROOT / \"sa-v\" / \"sav_val\" / \"Annotations_6fps\"\n",
        "\n",
        "SA1B_NUM_IMAGES = 8  # target range [5, 10]\n",
        "SAV_NUM_VIDEOS = 3   # target range [2, 3]\n",
        "POINT_SET_SIZES = [1, 3, 5]\n",
        "SA1B_PRIMARY_AREA_FRAC = 0.05\n",
        "SA1B_SECONDARY_AREA_FRAC = 0.02\n",
        "SAV_MIN_AREA_FRAC = 0.02\n",
        "SAV_MIN_MASK_FRAMES = 6\n",
        "SAV_TIMELINE_FRAMES = 6\n",
        "\n",
        "SA1B_NUM_IMAGES = min(10, max(5, SA1B_NUM_IMAGES))\n",
        "SAV_NUM_VIDEOS = min(3, max(2, SAV_NUM_VIDEOS))\n",
        "\n",
        "RNG = np.random.default_rng(GLOBAL_SEED)\n",
        "\n",
        "OUTPUT_DIR = Path(\"../qualitative_val_outputs\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR = OUTPUT_DIR / \"cache\"\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PDF_IMAGES_PATH = OUTPUT_DIR / \"qualitative_val_images.pdf\"\n",
        "PDF_VIDEOS_PATH = OUTPUT_DIR / \"qualitative_val_videos.pdf\"\n",
        "IMAGE_CACHE_PATH = CACHE_DIR / \"sa1b_preds.pkl\"\n",
        "VIDEO_CACHE_PATH = CACHE_DIR / \"sav_preds.pkl\"\n",
        "\n",
        "REFERENCE_COLOR = (0, 196, 255)\n",
        "PRED_COLOR = (255, 100, 100)\n",
        "GT_COLOR = (111, 255, 0)\n",
        "MASK_ALPHA = 0.55\n",
        "POINT_MARKER_SIZE = 150\n",
        "POINT_EDGE_COLOR = \"white\"\n",
        "POINT_COLOR = \"#00ff69\"\n",
        "POINT_LINEWIDTH = 1.1\n",
        "BASE_FIG_COLS = 4\n",
        "\n",
        "print(f\"SA-1B val root: {SA1B_VAL_ROOT}\")\n",
        "print(f\"SA-V val root : {SAV_VAL_FRAMES}\")\n",
        "print(f\"Image PDF : {PDF_IMAGES_PATH.resolve()}\")\n",
        "print(f\"Video PDF : {PDF_VIDEOS_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SA1BPromptSample:\n",
        "    sample_id: str\n",
        "    image_path: Path\n",
        "    json_path: Path\n",
        "    image_np: np.ndarray\n",
        "    gt_mask: np.ndarray\n",
        "    points_by_count: Dict[int, np.ndarray]\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SAVVideoSample:\n",
        "    sample_id: str\n",
        "    video_id: str\n",
        "    object_id: int\n",
        "    video_dir: Path\n",
        "    frame_indices: List[int]\n",
        "    frame_idx_to_path: Dict[int, Path]\n",
        "    prompt_frame_idx: int\n",
        "    prompt_mask: np.ndarray\n",
        "    prompt_points: np.ndarray\n",
        "    timeline_indices: List[int]\n",
        "    gt_masks_by_frame: Dict[int, np.ndarray]\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "def ensure_rle(segmentation, height: int, width: int) -> Dict:\n",
        "    if isinstance(segmentation, dict) and \"counts\" in segmentation:\n",
        "        rle = dict(segmentation)\n",
        "    elif isinstance(segmentation, list):\n",
        "        rles = mask_utils.frPyObjects(segmentation, height, width)\n",
        "        rle = mask_utils.merge(rles)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported segmentation format\")\n",
        "    if isinstance(rle.get(\"counts\"), bytes):\n",
        "        rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
        "    return rle\n",
        "\n",
        "\n",
        "def decode_rle_mask(rle: Dict) -> np.ndarray:\n",
        "    mask = mask_utils.decode([rle])[:, :, 0]\n",
        "    return mask.astype(bool)\n",
        "\n",
        "\n",
        "def sample_points_from_mask(mask: np.ndarray, num_points: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    ys, xs = np.nonzero(mask)\n",
        "    coords = np.stack([xs, ys], axis=1)\n",
        "    if len(coords) == 0:\n",
        "        raise ValueError(\"Empty mask cannot provide prompts\")\n",
        "    replace = len(coords) < num_points\n",
        "    idxs = rng.choice(len(coords), size=num_points, replace=replace)\n",
        "    return coords[idxs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_sa1b_entry(img_path: Path, rng: np.random.Generator, cache: Dict[Path, Dict]) -> Dict:\n",
        "    if img_path in cache:\n",
        "        return cache[img_path]\n",
        "\n",
        "    json_path = img_path.with_suffix(\".json\")\n",
        "    if not json_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing annotation for {img_path}\")\n",
        "\n",
        "    with Image.open(img_path) as img:\n",
        "        rgb = img.convert(\"RGB\")\n",
        "        image_np = np.array(rgb)\n",
        "        width, height = rgb.size\n",
        "\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    annotations = data[\"annotations\"] if isinstance(data, dict) and \"annotations\" in data else data\n",
        "\n",
        "    entry = {\n",
        "        \"image_np\": image_np,\n",
        "        \"annotations\": annotations,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"ann_order\": rng.permutation(len(annotations)).tolist() if annotations else [],\n",
        "        \"mask_cache\": {},\n",
        "    }\n",
        "    cache[img_path] = entry\n",
        "    return entry\n",
        "\n",
        "\n",
        "def _select_mask(entry: Dict, area_thresh: float) -> Optional[Tuple[int, np.ndarray, float]]:\n",
        "    if not entry[\"annotations\"]:\n",
        "        return None\n",
        "    area = entry[\"width\"] * entry[\"height\"]\n",
        "    for ann_idx in entry[\"ann_order\"]:\n",
        "        ann = entry[\"annotations\"][ann_idx]\n",
        "        seg = ann.get(\"segmentation\")\n",
        "        if seg is None:\n",
        "            continue\n",
        "        if ann_idx not in entry[\"mask_cache\"]:\n",
        "            try:\n",
        "                rle = ensure_rle(seg, entry[\"height\"], entry[\"width\"])\n",
        "                mask = decode_rle_mask(rle)\n",
        "            except Exception:\n",
        "                mask = None\n",
        "            entry[\"mask_cache\"][ann_idx] = mask\n",
        "        mask = entry[\"mask_cache\"].get(ann_idx)\n",
        "        if mask is None or mask.sum() == 0:\n",
        "            continue\n",
        "        area_frac = mask.sum() / max(area, 1)\n",
        "        if area_frac >= area_thresh:\n",
        "            return ann_idx, mask, area_frac\n",
        "    return None\n",
        "\n",
        "\n",
        "def sample_sa1b_prompts(root: Path, num_images: int, point_sizes: List[int], rng: np.random.Generator) -> List[SA1BPromptSample]:\n",
        "    image_paths = sorted(root.glob(\"*.jpg\"))\n",
        "    if not image_paths:\n",
        "        raise FileNotFoundError(f\"No SA-1B images found under {root}\")\n",
        "\n",
        "    target = min(len(image_paths), num_images)\n",
        "    order = rng.permutation(len(image_paths))\n",
        "    cache: Dict[Path, Dict] = {}\n",
        "    used_indices = set()\n",
        "    samples: List[SA1BPromptSample] = []\n",
        "\n",
        "    for area_thresh in [SA1B_PRIMARY_AREA_FRAC, SA1B_SECONDARY_AREA_FRAC]:\n",
        "        for idx in order:\n",
        "            if idx in used_indices:\n",
        "                continue\n",
        "            img_path = image_paths[idx]\n",
        "            json_path = img_path.with_suffix(\".json\")\n",
        "            if not json_path.exists():\n",
        "                continue\n",
        "            try:\n",
        "                entry = _load_sa1b_entry(img_path, rng, cache)\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "            selected = _select_mask(entry, area_thresh)\n",
        "            if selected is None:\n",
        "                continue\n",
        "            ann_idx, mask, area_frac = selected\n",
        "\n",
        "            points_by_count = {\n",
        "                count: sample_points_from_mask(mask, count, rng)\n",
        "                for count in point_sizes\n",
        "            }\n",
        "            samples.append(\n",
        "                SA1BPromptSample(\n",
        "                    sample_id=img_path.stem,\n",
        "                    image_path=img_path,\n",
        "                    json_path=json_path,\n",
        "                    image_np=entry[\"image_np\"],\n",
        "                    gt_mask=mask,\n",
        "                    points_by_count=points_by_count,\n",
        "                    metadata={\n",
        "                        \"ann_idx\": int(ann_idx),\n",
        "                        \"mask_area\": int(mask.sum()),\n",
        "                        \"mask_area_frac\": float(area_frac),\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "            used_indices.add(idx)\n",
        "            if len(samples) == target:\n",
        "                break\n",
        "        if len(samples) == target:\n",
        "            break\n",
        "\n",
        "    if len(samples) < target:\n",
        "        print(f\"[WARN] Requested {target} SA-1B samples but only collected {len(samples)}.\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "def _compute_timeline_indices(frame_indices: List[int], desired: int, ensure_idx: int) -> List[int]:\n",
        "    if not frame_indices:\n",
        "        return []\n",
        "    desired = max(1, min(desired, len(frame_indices)))\n",
        "    timeline = []\n",
        "    if ensure_idx in frame_indices:\n",
        "        timeline.append(ensure_idx)\n",
        "    positions = np.linspace(0, len(frame_indices) - 1, desired).astype(int)\n",
        "    for pos in positions:\n",
        "        idx = frame_indices[pos]\n",
        "        if idx not in timeline:\n",
        "            timeline.append(idx)\n",
        "        if len(timeline) == desired:\n",
        "            break\n",
        "    timeline.sort()\n",
        "    return timeline\n",
        "\n",
        "\n",
        "def sample_sav_videos(frames_root: Path, ann_root: Path, num_videos: int, rng: np.random.Generator) -> List[SAVVideoSample]:\n",
        "    video_dirs = sorted([p for p in frames_root.iterdir() if p.is_dir()])\n",
        "    if not video_dirs:\n",
        "        raise FileNotFoundError(f\"No SA-V videos found under {frames_root}\")\n",
        "\n",
        "    target = min(len(video_dirs), num_videos)\n",
        "    order = rng.permutation(len(video_dirs))\n",
        "    samples: List[SAVVideoSample] = []\n",
        "\n",
        "    for idx in order:\n",
        "        video_dir = video_dirs[idx]\n",
        "        video_id = video_dir.name\n",
        "        frame_idx_to_path = {}\n",
        "        for frame_path in sorted(video_dir.glob(\"*.jpg\")):\n",
        "            try:\n",
        "                frame_idx = int(frame_path.stem)\n",
        "            except ValueError:\n",
        "                continue\n",
        "            frame_idx_to_path[frame_idx] = frame_path\n",
        "        frame_indices = sorted(frame_idx_to_path)\n",
        "        if not frame_indices:\n",
        "            continue\n",
        "\n",
        "        ann_dir = ann_root / video_id\n",
        "        if not ann_dir.exists():\n",
        "            continue\n",
        "        obj_dirs = [p for p in ann_dir.iterdir() if p.is_dir()]\n",
        "        if not obj_dirs:\n",
        "            continue\n",
        "        obj_order = rng.permutation(len(obj_dirs))\n",
        "\n",
        "        chosen = None\n",
        "        for obj_idx in obj_order:\n",
        "            obj_dir = obj_dirs[obj_idx]\n",
        "            mask_entries: List[Tuple[int, np.ndarray]] = []\n",
        "            for mask_file in sorted(obj_dir.glob(\"*.png\")):\n",
        "                try:\n",
        "                    frame_idx = int(mask_file.stem)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "                if frame_idx not in frame_idx_to_path:\n",
        "                    continue\n",
        "                mask = np.array(Image.open(mask_file).convert(\"L\")) > 0\n",
        "                area_frac = mask.mean()\n",
        "                if area_frac < SAV_MIN_AREA_FRAC:\n",
        "                    continue\n",
        "                mask_entries.append((frame_idx, mask))\n",
        "            if len(mask_entries) < SAV_MIN_MASK_FRAMES:\n",
        "                continue\n",
        "            mask_entries.sort(key=lambda x: x[0])\n",
        "            chosen = (int(obj_dir.name), mask_entries)\n",
        "            break\n",
        "\n",
        "        if chosen is None:\n",
        "            continue\n",
        "\n",
        "        object_id, mask_entries = chosen\n",
        "        prompt_frame_idx, prompt_mask = mask_entries[0]\n",
        "        prompt_points = sample_points_from_mask(prompt_mask, min(5, max(1, prompt_mask.sum())), rng)\n",
        "        mask_frame_indices = [frame_idx for frame_idx, _ in mask_entries]\n",
        "        timeline = _compute_timeline_indices(mask_frame_indices, SAV_TIMELINE_FRAMES, prompt_frame_idx)\n",
        "        gt_masks_by_frame = {frame_idx: mask for frame_idx, mask in mask_entries}\n",
        "\n",
        "        samples.append(\n",
        "            SAVVideoSample(\n",
        "                sample_id=f\"{video_id}_obj{object_id:03d}\",\n",
        "                video_id=video_id,\n",
        "                object_id=object_id,\n",
        "                video_dir=video_dir,\n",
        "                frame_indices=frame_indices,\n",
        "                frame_idx_to_path=frame_idx_to_path,\n",
        "                prompt_frame_idx=prompt_frame_idx,\n",
        "                prompt_mask=prompt_mask,\n",
        "                prompt_points=prompt_points,\n",
        "                timeline_indices=timeline,\n",
        "                gt_masks_by_frame=gt_masks_by_frame,\n",
        "                metadata={\n",
        "                    \"area_frac\": float(prompt_mask.mean()),\n",
        "                    \"num_frames\": len(frame_indices),\n",
        "                    \"mask_frame_coverage\": len(mask_entries) / len(frame_indices),\n",
        "                },\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if len(samples) == target:\n",
        "            break\n",
        "\n",
        "    if len(samples) < target:\n",
        "        print(f\"[WARN] Requested {target} SA-V samples but only collected {len(samples)}.\")\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_prediction_cache(cache_path: Path, expected_metadata: Dict[str, Any]):\n",
        "    if not cache_path.exists():\n",
        "        return None\n",
        "    try:\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "    except Exception as exc:\n",
        "        print(f\"[cache] Failed to load {cache_path.name}: {exc}\")\n",
        "        return None\n",
        "    for key, value in expected_metadata.items():\n",
        "        if data.get(key) != value:\n",
        "            return None\n",
        "    return data.get(\"predictions\")\n",
        "\n",
        "\n",
        "def save_prediction_cache(cache_path: Path, metadata: Dict[str, Any], predictions: Dict[str, Any]):\n",
        "    payload = dict(metadata)\n",
        "    payload[\"predictions\"] = predictions\n",
        "    try:\n",
        "        with open(cache_path, \"wb\") as f:\n",
        "            pickle.dump(payload, f)\n",
        "    except Exception as exc:\n",
        "        print(f\"[cache] Failed to save {cache_path.name}: {exc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache-aware inference helper overrides\n",
        "\n",
        "def run_image_inference(model_specs: List[Dict], sa1b_samples: List[SA1BPromptSample], point_sizes: List[int]):\n",
        "    sample_ids = sorted(sample.sample_id for sample in sa1b_samples)\n",
        "    model_keys = sorted(spec[\"key\"] for spec in model_specs)\n",
        "    cache_meta = {\n",
        "        \"type\": \"sa1b\",\n",
        "        \"version\": 1,\n",
        "        \"sample_ids\": sample_ids,\n",
        "        \"model_keys\": model_keys,\n",
        "        \"point_sizes\": list(point_sizes),\n",
        "    }\n",
        "    cached = load_prediction_cache(IMAGE_CACHE_PATH, cache_meta)\n",
        "    if cached is not None:\n",
        "        print(f\"[cache] Using cached SA-1B predictions from {IMAGE_CACHE_PATH.name}\")\n",
        "        return cached\n",
        "\n",
        "    predictions = {sample.sample_id: {} for sample in sa1b_samples}\n",
        "    for spec in model_specs:\n",
        "        ckpt_path = Path(spec[\"checkpoint\"])\n",
        "        if not ckpt_path.exists():\n",
        "            print(f\"[SKIP] Missing checkpoint for {spec['label']}: {ckpt_path}\")\n",
        "            continue\n",
        "        print(f\"[Image] Loading {spec['label']} from {ckpt_path}\")\n",
        "        predictor = build_image_predictor(spec[\"model_cfg\"], spec[\"checkpoint\"])\n",
        "        for sample in sa1b_samples:\n",
        "            predictor.set_image(sample.image_np)\n",
        "            predictions[sample.sample_id][spec[\"key\"]] = {\n",
        "                count: predict_with_points(predictor, sample.points_by_count[count])\n",
        "                for count in point_sizes\n",
        "            }\n",
        "        del predictor\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    save_prediction_cache(IMAGE_CACHE_PATH, cache_meta, predictions)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_video_inference(model_specs: List[Dict], sav_samples: List[SAVVideoSample]):\n",
        "    sample_ids = sorted(sample.sample_id for sample in sav_samples)\n",
        "    model_keys = sorted(spec[\"key\"] for spec in model_specs)\n",
        "    cache_meta = {\n",
        "        \"type\": \"sav\",\n",
        "        \"version\": 1,\n",
        "        \"sample_ids\": sample_ids,\n",
        "        \"model_keys\": model_keys,\n",
        "    }\n",
        "    cached = load_prediction_cache(VIDEO_CACHE_PATH, cache_meta)\n",
        "    if cached is not None:\n",
        "        print(f\"[cache] Using cached SA-V predictions from {VIDEO_CACHE_PATH.name}\")\n",
        "        return cached\n",
        "\n",
        "    predictions = {sample.sample_id: {} for sample in sav_samples}\n",
        "    for spec in model_specs:\n",
        "        ckpt_path = Path(spec[\"checkpoint\"])\n",
        "        if not ckpt_path.exists():\n",
        "            print(f\"[SKIP] Missing checkpoint for {spec['label']}: {ckpt_path}\")\n",
        "            continue\n",
        "        print(f\"[Video] Loading {spec['label']} from {ckpt_path}\")\n",
        "        predictor = build_video_predictor(spec[\"model_cfg\"], spec[\"checkpoint\"])\n",
        "        for sample in sav_samples:\n",
        "            timeline = sample.timeline_indices or _compute_timeline_indices(\n",
        "                sample.frame_indices, SAV_TIMELINE_FRAMES, sample.prompt_frame_idx\n",
        "            )\n",
        "            timeline_set = set(timeline)\n",
        "            inference_state = predictor.init_state(str(sample.video_dir))\n",
        "            prompt_mask_tensor = torch.from_numpy(sample.prompt_mask.astype(np.uint8))\n",
        "            frame_idx, obj_ids, prompt_masks = predictor.add_new_mask(\n",
        "                inference_state=inference_state,\n",
        "                frame_idx=sample.prompt_frame_idx,\n",
        "                obj_id=sample.object_id,\n",
        "                mask=prompt_mask_tensor,\n",
        "            )\n",
        "            mask_records: Dict[int, np.ndarray] = {}\n",
        "            if frame_idx in timeline_set and sample.object_id in obj_ids:\n",
        "                obj_position = obj_ids.index(sample.object_id)\n",
        "                mask_records[frame_idx] = (\n",
        "                    prompt_masks[obj_position].squeeze().gt(0).cpu().numpy()\n",
        "                )\n",
        "            for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "                if not timeline_set:\n",
        "                    break\n",
        "                if out_frame_idx not in timeline_set:\n",
        "                    continue\n",
        "                for obj_position, obj_id in enumerate(out_obj_ids):\n",
        "                    if obj_id != sample.object_id:\n",
        "                        continue\n",
        "                    mask = out_mask_logits[obj_position].squeeze().gt(0).cpu().numpy()\n",
        "                    mask_records[out_frame_idx] = mask\n",
        "                if len(mask_records) == len(timeline_set):\n",
        "                    break\n",
        "            predictions[sample.sample_id][spec[\"key\"]] = {\n",
        "                \"timeline_indices\": timeline,\n",
        "                \"masks\": mask_records,\n",
        "            }\n",
        "        del predictor\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    save_prediction_cache(VIDEO_CACHE_PATH, cache_meta, predictions)\n",
        "    return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictor helpers\n",
        "\n",
        "def build_image_predictor(model_cfg_path: str, checkpoint_path: str) -> SAM2ImagePredictor:\n",
        "    model = build_sam2(model_cfg_path, checkpoint_path, device=device)\n",
        "    return SAM2ImagePredictor(model)\n",
        "\n",
        "\n",
        "def build_video_predictor(model_cfg_path: str, checkpoint_path: str):\n",
        "    return build_sam2_video_predictor(model_cfg_path, checkpoint_path, device=device)\n",
        "\n",
        "\n",
        "def predict_with_points(predictor: SAM2ImagePredictor, point_coords: np.ndarray) -> np.ndarray:\n",
        "    coords = np.array(point_coords, dtype=np.float32)\n",
        "    labels = np.ones(len(coords), dtype=np.int32)\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=coords,\n",
        "        point_labels=labels,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    return masks[0].astype(bool)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered 12 model variants for comparison.\n"
          ]
        }
      ],
      "source": [
        "MODEL_SPECS = [\n",
        "    {\n",
        "        \"key\": \"sam2_base_plus\",\n",
        "        \"label\": \"SAM2.1 B+\",\n",
        "        \"family\": \"sam2\",\n",
        "        \"scale\": \"base_plus\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
        "        \"checkpoint\": \"../checkpoints/sam2.1_hiera_base_plus.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"sam2_small\",\n",
        "        \"label\": \"SAM2.1 S\",\n",
        "        \"family\": \"sam2\",\n",
        "        \"scale\": \"small\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
        "        \"checkpoint\": \"../checkpoints/sam2.1_hiera_small.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"sam2_tiny\",\n",
        "        \"label\": \"SAM2.1 T\",\n",
        "        \"family\": \"sam2\",\n",
        "        \"scale\": \"tiny\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
        "        \"checkpoint\": \"../checkpoints/sam2.1_hiera_tiny.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"minmax_base_plus\",\n",
        "        \"label\": \"MinMax B+\",\n",
        "        \"family\": \"minmax\",\n",
        "        \"scale\": \"base_plus\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
        "        \"checkpoint\": \"../sam2_minmax/minmax_qat_base_plus_20251111_122542/checkpoints/checkpoint_sam2.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"minmax_small\",\n",
        "        \"label\": \"MinMax S\",\n",
        "        \"family\": \"minmax\",\n",
        "        \"scale\": \"small\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
        "        \"checkpoint\": \"../sam2_minmax/minmax_qat_small_20251111_233441/checkpoints/checkpoint_sam2.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"minmax_tiny\",\n",
        "        \"label\": \"MinMax T\",\n",
        "        \"family\": \"minmax\",\n",
        "        \"scale\": \"tiny\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
        "        \"checkpoint\": \"../sam2_minmax/minmax_qat_tiny_20251111_165611/checkpoints/checkpoint_sam2.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"baseonly_base_plus\",\n",
        "        \"label\": \"ALPQ_SAM2 B+\",\n",
        "        \"family\": \"baseonly\",\n",
        "        \"scale\": \"base_plus\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/ablations/adaptive_qat_toy_base_plus_20251112_101653/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"baseonly_main_small\",\n",
        "        \"label\": \"ALPQ_SAM2 S\",\n",
        "        \"family\": \"baseonly\",\n",
        "        \"scale\": \"small\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/main_small_20251113_212939/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"baseonly_main_tiny\",\n",
        "        \"label\": \"ALPQ_SAM2 T\",\n",
        "        \"family\": \"baseonly\",\n",
        "        \"scale\": \"tiny\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/main_tiny_20251113_212949/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"alpq_base_plus\",\n",
        "        \"label\": \"Explicit-ALPQ B+\",\n",
        "        \"family\": \"alpq\",\n",
        "        \"scale\": \"base_plus\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_base_plus_20251110_155500/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"alpq_small\",\n",
        "        \"label\": \"Explicit-ALPQ S\",\n",
        "        \"family\": \"alpq\",\n",
        "        \"scale\": \"small\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_small_20251111_172858/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "    {\n",
        "        \"key\": \"alpq_tiny\",\n",
        "        \"label\": \"Explicit-ALPQ T\",\n",
        "        \"family\": \"alpq\",\n",
        "        \"scale\": \"tiny\",\n",
        "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
        "        \"checkpoint\": \"../sam2_logs/classic/adaptive_qat_toy_tiny_20251112_161453_importancefixed/checkpoints/checkpoint.pt\",\n",
        "    },\n",
        "]\n",
        "\n",
        "SCALE_ORDER = [\"base_plus\", \"small\", \"tiny\"]\n",
        "SCALE_DISPLAY = {\"base_plus\": \"B+\", \"small\": \"S\", \"tiny\": \"T\"}\n",
        "FAMILY_ORDER = [\"sam2\", \"minmax\", \"baseonly\", \"alpq\"]\n",
        "FAMILY_DISPLAY = {\n",
        "    \"sam2\": \"SAM2\",\n",
        "    \"minmax\": \"MinMax\",\n",
        "    \"baseonly\": \"ALPQ\",\n",
        "    \"alpq\": \"Explicit-ALPQ\",\n",
        "}\n",
        "\n",
        "SPECS_BY_KEY = {spec[\"key\"]: spec for spec in MODEL_SPECS}\n",
        "SPECS_BY_SCALE = defaultdict(list)\n",
        "SPECS_BY_FAMILY = defaultdict(dict)\n",
        "for spec in MODEL_SPECS:\n",
        "    SPECS_BY_SCALE[spec[\"scale\"]].append(spec)\n",
        "    SPECS_BY_FAMILY[spec[\"family\"]][spec[\"scale\"]] = spec\n",
        "\n",
        "print(f\"Registered {len(MODEL_SPECS)} model variants for comparison.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override inference helpers with cache-aware versions\n",
        "\n",
        "def run_image_inference(model_specs: List[Dict], sa1b_samples: List[SA1BPromptSample], point_sizes: List[int]):\n",
        "    sample_ids = sorted(sample.sample_id for sample in sa1b_samples)\n",
        "    model_keys = sorted(spec[\"key\"] for spec in model_specs)\n",
        "    cache_meta = {\n",
        "        \"type\": \"sa1b\",\n",
        "        \"version\": 1,\n",
        "        \"sample_ids\": sample_ids,\n",
        "        \"model_keys\": model_keys,\n",
        "        \"point_sizes\": point_sizes,\n",
        "    }\n",
        "    cached = load_prediction_cache(IMAGE_CACHE_PATH, cache_meta)\n",
        "    if cached is not None:\n",
        "        print(f\"[cache] Using cached SA-1B predictions from {IMAGE_CACHE_PATH.name}\")\n",
        "        return cached\n",
        "\n",
        "    predictions = {sample.sample_id: {} for sample in sa1b_samples}\n",
        "    for spec in model_specs:\n",
        "        ckpt_path = Path(spec[\"checkpoint\"])\n",
        "        if not ckpt_path.exists():\n",
        "            print(f\"[SKIP] Missing checkpoint for {spec['label']}: {ckpt_path}\")\n",
        "            continue\n",
        "        print(f\"[Image] Loading {spec['label']} from {ckpt_path}\")\n",
        "        predictor = build_image_predictor(spec[\"model_cfg\"], spec[\"checkpoint\"])\n",
        "        for sample in sa1b_samples:\n",
        "            predictor.set_image(sample.image_np)\n",
        "            predictions[sample.sample_id][spec[\"key\"]] = {\n",
        "                count: predict_with_points(predictor, sample.points_by_count[count])\n",
        "                for count in point_sizes\n",
        "            }\n",
        "        del predictor\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    save_prediction_cache(IMAGE_CACHE_PATH, cache_meta, predictions)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_video_inference(model_specs: List[Dict], sav_samples: List[SAVVideoSample]):\n",
        "    sample_ids = sorted(sample.sample_id for sample in sav_samples)\n",
        "    model_keys = sorted(spec[\"key\"] for spec in model_specs)\n",
        "    cache_meta = {\n",
        "        \"type\": \"sav\",\n",
        "        \"version\": 1,\n",
        "        \"sample_ids\": sample_ids,\n",
        "        \"model_keys\": model_keys,\n",
        "    }\n",
        "    cached = load_prediction_cache(VIDEO_CACHE_PATH, cache_meta)\n",
        "    if cached is not None:\n",
        "        print(f\"[cache] Using cached SA-V predictions from {VIDEO_CACHE_PATH.name}\")\n",
        "        return cached\n",
        "\n",
        "    predictions = {sample.sample_id: {} for sample in sav_samples}\n",
        "    for spec in model_specs:\n",
        "        ckpt_path = Path(spec[\"checkpoint\"])\n",
        "        if not ckpt_path.exists():\n",
        "            print(f\"[SKIP] Missing checkpoint for {spec['label']}: {ckpt_path}\")\n",
        "            continue\n",
        "        print(f\"[Video] Loading {spec['label']} from {ckpt_path}\")\n",
        "        predictor = build_video_predictor(spec[\"model_cfg\"], spec[\"checkpoint\"])\n",
        "        for sample in sav_samples:\n",
        "            timeline = sample.timeline_indices or _compute_timeline_indices(\n",
        "                sample.frame_indices, SAV_TIMELINE_FRAMES, sample.prompt_frame_idx\n",
        "            )\n",
        "            target_frames = set(timeline)\n",
        "            remaining_frames = set(timeline)\n",
        "            inference_state = predictor.init_state(str(sample.video_dir))\n",
        "            prompt_mask_tensor = torch.from_numpy(sample.prompt_mask.astype(np.uint8))\n",
        "            frame_idx, obj_ids, prompt_masks = predictor.add_new_mask(\n",
        "                inference_state=inference_state,\n",
        "                frame_idx=sample.prompt_frame_idx,\n",
        "                obj_id=sample.object_id,\n",
        "                mask=prompt_mask_tensor,\n",
        "            )\n",
        "            mask_records: Dict[int, np.ndarray] = {}\n",
        "            if frame_idx in remaining_frames and sample.object_id in obj_ids:\n",
        "                obj_position = obj_ids.index(sample.object_id)\n",
        "                mask_records[frame_idx] = (\n",
        "                    prompt_masks[obj_position].squeeze().gt(0).cpu().numpy()\n",
        "                )\n",
        "                remaining_frames.discard(frame_idx)\n",
        "            for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "                if out_frame_idx in remaining_frames:\n",
        "                    for obj_position, obj_id in enumerate(out_obj_ids):\n",
        "                        if obj_id != sample.object_id:\n",
        "                            continue\n",
        "                        mask = out_mask_logits[obj_position].squeeze().gt(0).cpu().numpy()\n",
        "                        mask_records[out_frame_idx] = mask\n",
        "                        remaining_frames.discard(out_frame_idx)\n",
        "                        break\n",
        "            if remaining_frames:\n",
        "                missing = \", \".join(str(idx) for idx in sorted(remaining_frames))\n",
        "                print(f\"[warn] Missing {len(remaining_frames)} frames for {sample.sample_id}: {missing}\")\n",
        "            predictions[sample.sample_id][spec[\"key\"]] = {\n",
        "                \"timeline_indices\": timeline,\n",
        "                \"masks\": mask_records,\n",
        "            }\n",
        "        del predictor\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    save_prediction_cache(VIDEO_CACHE_PATH, cache_meta, predictions)\n",
        "    return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (legacy definitions removed to avoid overriding cache-aware helpers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override video timeline renderer for clearer left-side labels\n",
        "ROW_LABEL_X = -0.28\n",
        "\n",
        "\n",
        "def _draw_row_label(ax, text: str, x_offset: float = ROW_LABEL_X):\n",
        "    if not text:\n",
        "        return\n",
        "    ax.text(\n",
        "        x_offset,\n",
        "        0.5,\n",
        "        text,\n",
        "        transform=ax.transAxes,\n",
        "        rotation=90,\n",
        "        ha=\"center\",\n",
        "        va=\"center\",\n",
        "        fontsize=10,\n",
        "        fontweight=\"bold\",\n",
        "        clip_on=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def render_sav_video_timelines(sample: SAVVideoSample, sample_preds: Dict[str, Dict[str, Any]], pdf: PdfPages):\n",
        "    cache: Dict[int, np.ndarray] = {}\n",
        "    for scale in SCALE_ORDER:\n",
        "        row_specs = [spec for spec in SPECS_BY_SCALE[scale] if spec[\"key\"] in sample_preds]\n",
        "        if not row_specs:\n",
        "            continue\n",
        "        timeline = sample.timeline_indices\n",
        "        if not timeline:\n",
        "            continue\n",
        "        num_rows = 1 + len(row_specs)\n",
        "        num_cols = len(timeline)\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(3.2 * num_cols, 2.4 * num_rows))\n",
        "        if num_rows == 1:\n",
        "            axes = axes[np.newaxis, :]\n",
        "\n",
        "        _draw_row_label(axes[0, 0], \"GT\")\n",
        "        for col_idx, frame_idx in enumerate(timeline):\n",
        "            img = _load_frame_image(sample, frame_idx, cache)\n",
        "            gt_mask = sample.gt_masks_by_frame.get(frame_idx)\n",
        "            plot_panel(\n",
        "                axes[0, col_idx],\n",
        "                img,\n",
        "                mask=gt_mask,\n",
        "                mask_color=GT_COLOR,\n",
        "                points=sample.prompt_points if frame_idx == sample.prompt_frame_idx else None,\n",
        "                top_label=f\"Frame {frame_idx:05d}\",\n",
        "            )\n",
        "\n",
        "        for row_idx, spec in enumerate(row_specs, start=1):\n",
        "            _draw_row_label(axes[row_idx, 0], spec[\"label\"])\n",
        "            pred = sample_preds[spec[\"key\"]]\n",
        "            masks = pred.get(\"masks\", {})\n",
        "            for col_idx, frame_idx in enumerate(timeline):\n",
        "                img = _load_frame_image(sample, frame_idx, cache)\n",
        "                mask = masks.get(frame_idx)\n",
        "                plot_panel(\n",
        "                    axes[row_idx, col_idx],\n",
        "                    img,\n",
        "                    mask=mask,\n",
        "                    mask_color=PRED_COLOR,\n",
        "                    points=sample.prompt_points if frame_idx == sample.prompt_frame_idx else None,\n",
        "                )\n",
        "\n",
        "        fig.subplots_adjust(left=0.1, right=0.98, top=0.95, bottom=0.08, wspace=0.02, hspace=0.02)\n",
        "        pdf.savefig(fig, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def blend_mask(image_np: np.ndarray, mask: np.ndarray, color: tuple, alpha: float) -> np.ndarray:\n",
        "    overlay = image_np.astype(np.float32).copy()\n",
        "    mask_bool = mask.astype(bool)\n",
        "    color_arr = np.array(color, dtype=np.float32)\n",
        "    overlay[mask_bool] = overlay[mask_bool] * (1 - alpha) + color_arr * alpha\n",
        "    return overlay.astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_points(ax, points: Optional[np.ndarray]):\n",
        "    if points is None or len(points) == 0:\n",
        "        return\n",
        "    pts = np.array(points)\n",
        "    ax.scatter(\n",
        "        pts[:, 0],\n",
        "        pts[:, 1],\n",
        "        s=POINT_MARKER_SIZE,\n",
        "        c=POINT_COLOR,\n",
        "        marker=\"*\",\n",
        "        edgecolors=POINT_EDGE_COLOR,\n",
        "        linewidths=POINT_LINEWIDTH,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_panel(\n",
        "    ax,\n",
        "    image_np: np.ndarray,\n",
        "    mask: Optional[np.ndarray] = None,\n",
        "    mask_color: tuple = PRED_COLOR,\n",
        "    points: Optional[np.ndarray] = None,\n",
        "    top_label: str = \"\",\n",
        "    bottom_label: str = \"\",\n",
        "):\n",
        "    disp = image_np\n",
        "    if mask is not None:\n",
        "        disp = blend_mask(image_np, mask, mask_color, MASK_ALPHA)\n",
        "    ax.imshow(disp)\n",
        "    plot_points(ax, points)\n",
        "    if top_label:\n",
        "        ax.set_title(top_label, fontsize=25, fontweight=\"bold\")\n",
        "    if bottom_label:\n",
        "        ax.text(\n",
        "            0.5,\n",
        "            -0.06,\n",
        "            bottom_label,\n",
        "            ha=\"center\",\n",
        "            va=\"top\",\n",
        "            fontsize=15,\n",
        "            transform=ax.transAxes,\n",
        "        )\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "\n",
        "def render_sa1b_one_point(sample: SA1BPromptSample, sample_preds: Dict[str, Dict[int, np.ndarray]], pdf: PdfPages):\n",
        "    families = [\n",
        "        fam\n",
        "        for fam in FAMILY_ORDER\n",
        "        if any(\n",
        "            (spec := SPECS_BY_FAMILY[fam].get(scale)) and spec[\"key\"] in sample_preds\n",
        "            for scale in SCALE_ORDER\n",
        "        )\n",
        "    ]\n",
        "    if not families:\n",
        "        return\n",
        "    scales = [\n",
        "        scale\n",
        "        for scale in SCALE_ORDER\n",
        "        if any((spec := SPECS_BY_FAMILY[fam].get(scale)) and spec[\"key\"] in sample_preds for fam in families)\n",
        "    ]\n",
        "    if not scales:\n",
        "        return\n",
        "\n",
        "    num_rows = len(families)\n",
        "    num_cols = len(scales) + 1  # reference + scales\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(3.0 * num_cols, 2.8 * num_rows))\n",
        "    if num_rows == 1:\n",
        "        axes = axes[np.newaxis, :]\n",
        "    for row_idx, fam in enumerate(families):\n",
        "        axes[row_idx, 0].set_ylabel(FAMILY_DISPLAY[fam], rotation=90, fontsize=10, fontweight=\"bold\", labelpad=18)\n",
        "        plot_panel(\n",
        "            axes[row_idx, 0],\n",
        "            sample.image_np,\n",
        "            mask=sample.gt_mask,\n",
        "            mask_color=GT_COLOR,\n",
        "            points=sample.points_by_count[1],\n",
        "            top_label=\"GT\" if row_idx == 0 else \"\",\n",
        "            bottom_label=\"GT\",\n",
        "        )\n",
        "        for col_offset, scale in enumerate(scales, start=1):\n",
        "            spec = SPECS_BY_FAMILY[fam].get(scale)\n",
        "            ax = axes[row_idx, col_offset]\n",
        "            if spec and spec[\"key\"] in sample_preds:\n",
        "                plot_panel(\n",
        "                    ax,\n",
        "                    sample.image_np,\n",
        "                    mask=sample_preds[spec[\"key\"]][1],\n",
        "                    mask_color=PRED_COLOR,\n",
        "                    points=sample.points_by_count[1],\n",
        "                    top_label=(f\"{SCALE_DISPLAY[scale]}\" if row_idx == 0 else \"\"),\n",
        "                    bottom_label=spec[\"label\"],\n",
        "                )\n",
        "            else:\n",
        "                ax.axis(\"off\")\n",
        "                if row_idx == 0:\n",
        "                    ax.set_title(f\"{SCALE_DISPLAY[scale]} (missing)\", fontsize=25, fontweight=\"bold\")\n",
        "\n",
        "    fig.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.08, wspace=0.05, hspace=0.05)\n",
        "    fig.tight_layout()\n",
        "    pdf.savefig(fig, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def render_sa1b_multi_scale(\n",
        "    sample: SA1BPromptSample,\n",
        "    sample_preds: Dict[str, Dict[int, np.ndarray]],\n",
        "    scale: str,\n",
        "    pdf: PdfPages,\n",
        "):\n",
        "    families = [\n",
        "        fam\n",
        "        for fam in FAMILY_ORDER\n",
        "        if (spec := SPECS_BY_FAMILY[fam].get(scale)) and spec[\"key\"] in sample_preds\n",
        "    ]\n",
        "    if not families:\n",
        "        return\n",
        "\n",
        "    num_rows = 1 + len(families)\n",
        "    num_cols = len(POINT_SET_SIZES)\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(3.0 * num_cols, 2.8 * num_rows))\n",
        "    if num_rows == 1:\n",
        "        axes = axes[np.newaxis, :]\n",
        "\n",
        "    for col_idx, count in enumerate(POINT_SET_SIZES):\n",
        "        plot_panel(\n",
        "            axes[0, col_idx],\n",
        "            sample.image_np,\n",
        "            mask=sample.gt_mask,\n",
        "            mask_color=GT_COLOR,\n",
        "            points=sample.points_by_count[count],\n",
        "            top_label=f\"{count} pt\",\n",
        "            bottom_label=\"GT\",\n",
        "        )\n",
        "\n",
        "    for row_idx, fam in enumerate(families, start=1):\n",
        "        spec = SPECS_BY_FAMILY[fam][scale]\n",
        "        axes[row_idx, 0].set_ylabel(FAMILY_DISPLAY[fam], rotation=90, fontsize=15, fontweight=\"bold\", labelpad=18)\n",
        "        for col_idx, count in enumerate(POINT_SET_SIZES):\n",
        "            plot_panel(\n",
        "                axes[row_idx, col_idx],\n",
        "                sample.image_np,\n",
        "                mask=sample_preds[spec[\"key\"]][count],\n",
        "                mask_color=PRED_COLOR,\n",
        "                points=sample.points_by_count[count],\n",
        "                bottom_label=spec[\"label\"],\n",
        "            )\n",
        "\n",
        "    fig.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.08, wspace=0.05, hspace=0.05)\n",
        "    fig.tight_layout()\n",
        "    pdf.savefig(fig, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def _load_frame_image(sample: SAVVideoSample, frame_idx: int, cache: Dict[int, np.ndarray]) -> np.ndarray:\n",
        "    if frame_idx in cache:\n",
        "        return cache[frame_idx]\n",
        "    path = sample.frame_idx_to_path.get(frame_idx)\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(f\"Missing frame {frame_idx} for {sample.sample_id}\")\n",
        "    img = np.array(Image.open(path).convert(\"RGB\"))\n",
        "    cache[frame_idx] = img\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_cover_page(pdf: PdfPages, title: str, sa1b_count: int, sav_count: int):\n",
        "    fig = plt.figure(figsize=(8.27, 11.69))\n",
        "    fig.text(0.05, 0.95, title, fontsize=24, weight=\"bold\")\n",
        "    fig.text(0.05, 0.91, f\"Generated: {datetime.now().isoformat(timespec='seconds')}\", fontsize=12)\n",
        "    fig.text(0.05, 0.87, f\"SA-1B samples: {sa1b_count} | SA-V videos: {sav_count}\", fontsize=14)\n",
        "    fig.text(0.05, 0.83, f\"Point settings: {POINT_SET_SIZES}\", fontsize=12)\n",
        "    fig.text(0.05, 0.79, \"Models\", fontsize=14, weight=\"bold\")\n",
        "\n",
        "    y = 0.76\n",
        "    for spec in MODEL_SPECS:\n",
        "        fig.text(0.06, y, f\"- {spec['label']} ({FAMILY_DISPLAY[spec['family']]} {SCALE_DISPLAY[spec['scale']]})\", fontsize=11)\n",
        "        y -= 0.02\n",
        "\n",
        "    pdf.savefig(fig, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 8 SA-1B samples and 3 SA-V videos.\n"
          ]
        }
      ],
      "source": [
        "sa1b_samples = sample_sa1b_prompts(SA1B_VAL_ROOT, SA1B_NUM_IMAGES, POINT_SET_SIZES, RNG)\n",
        "sav_samples = sample_sav_videos(SAV_VAL_FRAMES, SAV_VAL_ANN, SAV_NUM_VIDEOS, RNG)\n",
        "\n",
        "if not sa1b_samples:\n",
        "    raise RuntimeError(\"SA-1B 샘플을 찾지 못했습니다. 경로 설정을 확인하세요.\")\n",
        "if not sav_samples:\n",
        "    raise RuntimeError(\"SA-V 샘플을 찾지 못했습니다. 경로 설정을 확인하세요.\")\n",
        "\n",
        "print(f\"Collected {len(sa1b_samples)} SA-1B samples and {len(sav_samples)} SA-V videos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"[Images] Running SA-1B qualitative inference ...\")\n",
        "# sa1b_preds = run_image_inference(MODEL_SPECS, sa1b_samples, POINT_SET_SIZES)\n",
        "# with PdfPages(PDF_IMAGES_PATH) as pdf:\n",
        "#     add_cover_page(pdf, \"SA-1B Qualitative\", len(sa1b_samples), 0)\n",
        "#     for sample in sa1b_samples:\n",
        "#         render_sa1b_one_point(sample, sa1b_preds.get(sample.sample_id, {}), pdf)\n",
        "#         for scale in SCALE_ORDER:\n",
        "#             render_sa1b_multi_scale(sample, sa1b_preds.get(sample.sample_id, {}), scale, pdf)\n",
        "# print(f\"[Images] Saved SA-1B PDF to {PDF_IMAGES_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Videos] Running SA-V qualitative inference ...\n",
            "[cache] Using cached SA-V predictions from sav_preds.pkl\n",
            "[Videos] Saved SA-V PDF to /home/lji/SAM/sam2/qualitative_val_outputs/qualitative_val_videos.pdf\n"
          ]
        }
      ],
      "source": [
        "print(\"[Videos] Running SA-V qualitative inference ...\")\n",
        "sav_preds = run_video_inference(MODEL_SPECS, sav_samples)\n",
        "with PdfPages(PDF_VIDEOS_PATH) as pdf:\n",
        "    add_cover_page(pdf, \"SA-V Qualitative\", 0, len(sav_samples))\n",
        "    for sample in sav_samples:\n",
        "        render_sav_video_timelines(sample, sav_preds.get(sample.sample_id, {}), pdf)\n",
        "print(f\"[Videos] Saved SA-V PDF to {PDF_VIDEOS_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SAM2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
