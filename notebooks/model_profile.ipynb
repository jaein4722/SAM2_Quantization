{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Footprint & Throughput Profiler\n",
    "\n",
    "Adaptive QAT / Min-Max QAT \ub4f1 \ub2e4\uc591\ud55c \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \ubd88\ub7ec\uc640\uc11c\n",
    "\n",
    "* \ud30c\ub77c\ubbf8\ud130 \uc218\n",
    "* \uac00\uc911\uce58 \ube44\ud2b8 \uae30\ubc18 \uc608\uc0c1 \ubaa8\ub378 \uc0ac\uc774\uc988\n",
    "* FPS / Latency\n",
    "\n",
    "\ub97c \ube44\uad50\ud558\uae30 \uc704\ud55c \ud15c\ud50c\ub9bf\uc785\ub2c8\ub2e4. \uc544\ub798 \uc124\uc815 \ube14\ub85d\uc744 \uc790\uc2e0\uc758 \uc2e4\ud5d8 \uacbd\ub85c\uc5d0 \ub9de\uac8c \uc218\uc815\ud55c \ub4a4 \uc140\uc744 \uc21c\uc11c\ub300\ub85c \uc2e4\ud589\ud558\uc138\uc694."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ef955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# \ub178\ud2b8\ubd81\uc744 sam2/notebooks \ud3f4\ub354\uc5d0\uc11c \uc2e4\ud589\ud558\ub294 \uacbd\uc6b0, sam2 \uc0c1\uc704\ub85c \uc774\ub3d9\n",
    "if Path.cwd().name == \"notebooks\":\n",
    "    REPO_ROOT = Path.cwd().parent\n",
    "    %cd ..\n",
    "else:\n",
    "    REPO_ROOT = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== \uc0ac\uc6a9\uc790 \uc124\uc815 ====\n",
    "# ckpt_path\ub294 \uc2e4\uc81c \uccb4\ud06c\ud3ec\uc778\ud2b8 \uacbd\ub85c\ub85c \uad50\uccb4\ud558\uc138\uc694.\n",
    "\n",
    "CHECKPOINTS = [\n",
    "    {\n",
    "        \"name\": \"original_base_plus\",\n",
    "        \"ckpt_path\": \"checkpoints/sam2.1_hiera_base_plus.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"bitwidth_key\": None,  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Base plus model\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"original_small\",\n",
    "        \"ckpt_path\": \"checkpoints/sam2.1_hiera_small.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"bitwidth_key\": None,  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Small model\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"original_tiny\",\n",
    "        \"ckpt_path\": \"checkpoints/sam2.1_hiera_tiny.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"bitwidth_key\": None,  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Tiny model\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ours_base_plus\",\n",
    "        \"ckpt_path\": \"sam2_logs/adaptive_qat_toy_20251110_155500/checkpoints/checkpoint.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"bitwidth_key\": \"bitwidths\",  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Base plus model with adaptive QAT\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ours_small\",\n",
    "        \"ckpt_path\": \"sam2_logs/adaptive_qat_toy_small_20251111_172858/checkpoints/checkpoint.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"bitwidth_key\": \"bitwidths\",  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Small model with adaptive QAT\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ours_tiny\",\n",
    "        \"ckpt_path\": \"sam2_logs/adaptive_qat_toy_tiny_20251111_173031/checkpoints/checkpoint.pt\",\n",
    "        \"model_cfg\": \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"bitwidth_key\": \"bitwidths\",  # state dict \uc548\uc5d0\uc11c layer\ubcc4 \ube44\ud2b8\ub97c \ub2f4\uace0 \uc788\ub294 \ud0a4 (\uc5c6\uc73c\uba74 None)\n",
    "        \"notes\": \"Tiny model with adaptive QAT\"\n",
    "    },\n",
    "]\n",
    "\n",
    "DEVICE = \"cuda:7\"\n",
    "BITWIDTH_KEY_FALLBACK = None  # \ubaa8\ub4e0 entry\uc5d0 \uacf5\ud1b5 \ud0a4\uac00 \uc788\ub2e4\uba74 \uc9c0\uc815\n",
    "WARMUP_RUNS = 5\n",
    "TIMED_RUNS = 20\n",
    "IMAGE_SIZE = 1024  # FPS \uce21\uc815 \uc2dc \uc0ac\uc6a9\ud560 \uc785\ub825 \ud574\uc0c1\ub3c4\n",
    "\n",
    "SUMMARY_RESULTS: list[Dict[str, Any]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_IMAGES = sorted([\n",
    "    *Path('../datasets/benchmark/coco/images').glob('*.jpg'),\n",
    "    *Path('../datasets/benchmark/sa1b').glob('*.jpg'),\n",
    "])\n",
    "BENCHMARK_VIDEO_FRAMES = []\n",
    "for video_dir in Path('../datasets/benchmark/sav/JPEGImages_24fps').glob('sav_*'):\n",
    "    frames = sorted(video_dir.glob('*.jpg'))\n",
    "    BENCHMARK_VIDEO_FRAMES.extend(frames[:3])  # first 3 frames per video\n",
    "BENCHMARK_IMGS = [Path(p) for p in BENCHMARK_IMAGES + BENCHMARK_VIDEO_FRAMES]\n",
    "print(f'Benchmark images collected: {len(BENCHMARK_IMGS)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_bits = []\n",
    "NON_QUANTIZED_MODULES = {\"image_encoder.trunk.pos_embed\", \"image_encoder.trunk.pos_embed_window\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_CONTROLLER_PREFIX = \"bit_controller.\"\n",
    "BIT_CONTROLLER_DOT_TOKEN = \"_DOT_\"\n",
    "\n",
    "\n",
    "def _resolve_model_state(state: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    candidates = (\"model\", \"student\", \"ema\", \"state_dict\", \"module\")\n",
    "    for key in candidates:\n",
    "        value = state.get(key)\n",
    "        if isinstance(value, dict):\n",
    "            return value\n",
    "    return {k: v for k, v in state.items() if torch.is_tensor(v)}\n",
    "\n",
    "\n",
    "def _merge_bit_controller_state(state: Dict[str, Any], model_state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    merged = dict(model_state)\n",
    "    def _inject(source):\n",
    "        if not isinstance(source, dict):\n",
    "            return\n",
    "        for key, value in source.items():\n",
    "            if isinstance(key, str) and key.startswith(BIT_CONTROLLER_PREFIX) and torch.is_tensor(value):\n",
    "                merged[key] = value\n",
    "    _inject(state.get('bit_controller'))\n",
    "    _inject(state.get('model_aux'))\n",
    "    return merged\n",
    "\n",
    "\n",
    "def _extract_bits_from_bit_controller(model_state: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "    bit_map: Dict[str, float] = {}\n",
    "    for name, tensor in model_state.items():\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            continue\n",
    "        if not name.startswith(BIT_CONTROLLER_PREFIX):\n",
    "            continue\n",
    "        suffix = name[len(BIT_CONTROLLER_PREFIX):]\n",
    "        if suffix.startswith(\"_\"):\n",
    "            continue\n",
    "        try:\n",
    "            value = float(tensor.detach().cpu().mean().item())\n",
    "        except Exception:\n",
    "            continue\n",
    "        layer_name = suffix.replace(BIT_CONTROLLER_DOT_TOKEN, \".\")\n",
    "        bit_map[layer_name] = value\n",
    "    return bit_map\n",
    "\n",
    "\n",
    "def _get_bitwidth_map(state: Dict[str, Any], model_state: Dict[str, torch.Tensor], entry: Dict[str, Any]) -> Dict[str, float]:\n",
    "    bit_map = _extract_bits_from_bit_controller(model_state)\n",
    "    if bit_map:\n",
    "        return bit_map\n",
    "    key = entry.get(\"bitwidth_key\") or BITWIDTH_KEY_FALLBACK\n",
    "    if key and key in state and isinstance(state[key], dict):\n",
    "        return state[key]\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _bytes_to_megabytes(num_bytes: float) -> float:\n",
    "    return num_bytes / (1024 ** 2)\n",
    "\n",
    "\n",
    "def summarize_checkpoint(entry: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ckpt_path = Path(entry[\"ckpt_path\"]).expanduser()\n",
    "    if not ckpt_path.is_file():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {ckpt_path}\")\n",
    "\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model_state = _merge_bit_controller_state(state, _resolve_model_state(state))\n",
    "    bitwidth_map = _get_bitwidth_map(state, model_state, entry) or {}\n",
    "    non_quantized = set(NON_QUANTIZED_MODULES)\n",
    "\n",
    "    default_w_bits = entry.get(\"default_bits\", {}).get(\"weight\", 32)\n",
    "    total_params = 0\n",
    "    total_bits = 0.0\n",
    "\n",
    "    for name, tensor in model_state.items():\n",
    "        if not torch.is_tensor(tensor):\n",
    "            continue\n",
    "        if name.startswith(BIT_CONTROLLER_PREFIX):\n",
    "            continue\n",
    "        numel = tensor.numel()\n",
    "        base_name = name\n",
    "        for suffix in (\".weight\", \".bias\"):\n",
    "            if base_name.endswith(suffix):\n",
    "                base_name = base_name[:-len(suffix)]\n",
    "                break\n",
    "        total_params += numel\n",
    "        layer_bits = bitwidth_map.get(name)\n",
    "        candidate = base_name\n",
    "        while layer_bits is None and '.' in candidate:\n",
    "            candidate = candidate.rsplit('.', 1)[0]\n",
    "            layer_bits = bitwidth_map.get(candidate)\n",
    "        if layer_bits is None and base_name in non_quantized:\n",
    "            bits = float(default_w_bits)\n",
    "        elif layer_bits is None:\n",
    "            bits = float(default_w_bits)\n",
    "        else:\n",
    "            bits = float(layer_bits)\n",
    "        total_bits += numel * bits\n",
    "\n",
    "    fp32_size_mb = _bytes_to_megabytes(total_params * 4)\n",
    "    avg_bits = (total_bits / max(total_params, 1)) if total_params else float(default_w_bits)\n",
    "    est_size_mb = _bytes_to_megabytes(total_bits / 8)\n",
    "\n",
    "    summary = {\n",
    "        \"name\": entry[\"name\"],\n",
    "        \"params\": total_params,\n",
    "        \"fp32_size_mb\": fp32_size_mb,\n",
    "        \"est_quant_size_mb\": est_size_mb,\n",
    "        \"avg_weight_bits\": avg_bits,\n",
    "        \"default_weight_bits\": default_w_bits,\n",
    "        \"parsed_bit_layers\": len(bitwidth_map),\n",
    "        \"notes\": entry.get(\"notes\", \"\")\n",
    "    }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RESULTS = [summarize_checkpoint(entry) for entry in CHECKPOINTS]\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    display(pd.DataFrame(SUMMARY_RESULTS))\n",
    "except ImportError:\n",
    "    SUMMARY_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sync(device: str) -> None:\n",
    "    if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize(torch.device(device))\n",
    "\n",
    "def build_predictor(entry: Dict[str, Any], device: str = DEVICE) -> SAM2ImagePredictor:\n",
    "    predictor_model = build_sam2(\n",
    "        config_file=entry[\"model_cfg\"],\n",
    "        ckpt_path=entry[\"ckpt_path\"],\n",
    "        device=device,\n",
    "        apply_postprocessing=True,\n",
    "    )\n",
    "    return SAM2ImagePredictor(predictor_model)\n",
    "\n",
    "def benchmark_entry(entry: Dict[str, Any], *, device: str = DEVICE, image_size: int = IMAGE_SIZE,\n",
    "                    warmup: int = WARMUP_RUNS, runs: int = TIMED_RUNS) -> Dict[str, Any]:\n",
    "    predictor = build_predictor(entry, device=device)\n",
    "\n",
    "    dummy_image = (np.random.rand(image_size, image_size, 3) * 255).astype(np.uint8)\n",
    "    predictor.set_image(dummy_image)\n",
    "    full_box = np.array([0, 0, image_size, image_size])\n",
    "\n",
    "    for _ in range(max(1, warmup)):\n",
    "        predictor.predict(box=full_box, multimask_output=False)\n",
    "\n",
    "    timings = []\n",
    "    for _ in range(max(1, runs)):\n",
    "        _sync(device)\n",
    "        start = time.perf_counter()\n",
    "        predictor.predict(box=full_box, multimask_output=False)\n",
    "        _sync(device)\n",
    "        timings.append(time.perf_counter() - start)\n",
    "\n",
    "    avg_latency = float(np.mean(timings))\n",
    "    fps = float(1.0 / avg_latency) if avg_latency > 0 else float(\"inf\")\n",
    "    return {\n",
    "        \"name\": entry[\"name\"],\n",
    "        \"latency_ms\": avg_latency * 1000,\n",
    "        \"fps\": fps,\n",
    "        \"image_size\": image_size,\n",
    "        \"runs\": runs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = []\n",
    "\n",
    "if DEVICE.startswith(\"cuda\") and not torch.cuda.is_available():\n",
    "    print(f\"CUDA \ub514\ubc14\uc774\uc2a4({DEVICE})\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc5b4 FPS \uce21\uc815\uc744 \uac74\ub108\ub701\ub2c8\ub2e4.\")\n",
    "else:\n",
    "    for entry in CHECKPOINTS:\n",
    "        try:\n",
    "            benchmark_results.append(benchmark_entry(entry))\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] {entry['name']} \uce21\uc815 \uc2e4\ud328: {exc}\")\n",
    "\n",
    "if benchmark_results:\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        display(pd.DataFrame(benchmark_results))\n",
    "    except ImportError:\n",
    "        benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}